% clear;
% %% % Generated by Jinsung Yoon (28th June 2016)
% % IUP + Contextual Hedge/Weighted Majority (Hedged Bandit)
% %% Data Input
% % Data (569 patients, 30 features, and 1 label)
% Data = csvread('Medical_Dataset.csv');
% 
% % Patient's Feature
% Feature = Data(:,1:30);
% % Patient's Label (Malignant = 1, Benign = 2)
% % Price Label (up=2, down=1)
% Label = Data(:,31);
function [ Result, weightobs,F_EL,P,predict_output,partition,score ] = Contextual_EL_WM(Feature, Label,Train_Temp,Test_Temp, No_E, strategy,ml,LLpred,kau)

% Feature Normalization
% for i = 1:30
%     Temp1 = max(Feature(:,i));
%     Feature(:,i) = Feature(:,i)/Temp1;
% end

%% System Variable
K = 1;     % Number of folds
T = length(Test_Temp);      % Number of Timesteps
%No_E = 3;       % Number of Experts
No_F_EL = 3;    % Number of EL features
feature_num_BL=size(Feature,2);
% Parameters
% For CUP
m_t = 2;    % Number of adaptive partition of each feature
% Coefficient of the control function. Co*(t^(d_t))*log(t)
d_t = 0;
Co = 0.1;   

% For Hedge
kau = 0.75;    % Punishment Constant for Hedge

%% Fixed System Variable
 [Patient_No ,Feature_No] = size(Feature);

% Train_Rate = 0.8;
% Train_No = ceil(Patient_No * Train_Rate);
Train_No=length(Train_Temp);
% Test_No = Patient_No - Train_No;
Test_No=length(Test_Temp);
% Action_No = 2;  % Malignant and Benign
score=zeros(length(Test_No),No_E+1);

%% Algorithm
% Iterative fold cross validation
for l = 1:K
    l
    %% Input Feature Randomization
    %% Train Data Randomize
%     Train_Temp = datasample([1:Patient_No],Train_No);
    Train_Feature = Feature(Train_Temp,:);
    Train_Label = Label(Train_Temp);
    
    %% Test Data Randomize
%     Test_Temp = setdiff([1:Patient_No],Train_Temp); % Excluding Train Data
    Test_Feature = Feature(Test_Temp,:);
    Test_Label = Label(Test_Temp);
    
    %% Test Data Generation (10,000 instances)
%     for t = 1:T
%         pat = round(Test_No*rand()+0.5);
%         Test_tmp(t)=pat;
%         Test_Feature(t,:) = Test_Feature_Set(pat,:);
%         Test_Label(t,:) = Test_Label_Set(pat);
%     end
    
    %% Assigned features for each expert without overlapping
    Temp_Select=randperm(feature_num_BL);
    for e = 1:No_E
        %F(e,:) = Temp_Select(((Feature_No/No_E)*e-((Feature_No/No_E)-1)):((Feature_No/No_E)*e));
        F(e,:)=1:size(Train_Feature,2);
    end
        
    %% Assigned features for EL
    %F_EL=datasample(unique(reshape(F,1,[])), No_F_EL, 'Replace',false);
    F_EL=[1,2,3];
    
    
    %% Create Basic Matrix
%     % For CUP
%     S_T = zeros(No_E,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,Action_No);    % Sum of rewards for each partition
%     N_T = zeros(No_E,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,Action_No);    % Number of patients in each partition
%     R_T = zeros(No_E,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,m_t,Action_No);    % Sample Rewards for each partition
    
    % For Adaptive Partitioning
    S = zeros(Feature_No,2);    % Sum of feature values for Malignant and Benign patients
    R = zeros(Feature_No,2);    % Sample feature values for Malignant and Benign patients
    N = zeros(2,1);     % Number of Benign or Malignat patients
    P = zeros(Feature_No,1);    % Boundary of Adaptive Partitioning
        
    % Prediction of each expert
    y_hat = zeros(T,No_E);
    
    %% Initialization
    w = (1/(No_E))*ones(2,2,2,No_E);    % Weights of each experts
    final_y = zeros(T,1);    % Ensemble Learner Estimate 
    final = zeros(T,1);    % Ensemble Learner prediction
    
    % For Performance Evaluation
    Count = zeros(5,1);     % For ensemble learner
    Counting = zeros(5,No_E);   % For average, best and worst learner
        
    %% Training
    %% Adaptive Partitioning
    for i = 1:Train_No
        if (Train_Label(i) == 1)    % For Malignant
            N(1) = N(1) + 1;
            for f = 1:Feature_No
                S(f,1) = S(f,1) + Train_Feature(i,f);
                R(f,1) = S(f,1)/N(1);
            end
        else
            % For Benign
            N(2) = N(2) + 1;
            for f = 1:Feature_No
                S(f,2) = S(f,2) + Train_Feature(i,f);
                R(f,2) = S(f,2)/N(2);
            end
        end
        % Update the boundary of partition
        for f = 1:Feature_No
            P(f) = (R(f,1)+R(f,2))/2;
        end
    end
    debug=0;
if debug==0,   
    for t = 1:Train_No
        % Exploitation/Exploration Flag for each Expert
        Flag = zeros(No_E,1);

        

        %% 1. Adaptive Partitioning
        for e = 1:No_E
            % Find P_T of X_T
%            for k = 1:(Feature_No/No_E)
            % Label partition for all feature arriving to each experts, who
            % observed partial features
            for k = 1:size(Train_Feature,2)
                if(Train_Feature(t,F(e,k)) < P(F(e,k)))     % The boundary is P(f), Updating Section is positioned at the end of the algorithm
                    x(t,F(e,k)) = 1;
                else
                    x(t,F(e,k)) = 2;
                end
            end
        end
        

        if ml==false,   
%             X=Train_Feature(1:t-1,:);
%             Y=Train_Label(1:t-1,:);
%             xtest=Train_Feature(t,:);
%             
%             [lasB, info] = lasso(X,Y,'NumLambda', 40);
%             yhatlas=info.Intercept(5)+xtest*lasB(:,5);
%             rfrB = TreeBagger(10,X,Y,'Method','regression'); 
%             Y=categorical(Y);
%             
%             logB=glmfit(X,Y,'binomial','link','logit');
%             %SVM better with categorical
%             svmB = fitcsvm(X,Y,'KernelFunction','rbf');
%             svmB = fitPosterior(svmB);
%             predlog=glmval(logB, xtest,'logit');
% 
%             y_hat(t,1)=Train_Label(t);
%             y_hat(t,2)=predict(svmB, xtest);
%             y_hat(t,3)=(predict(rfrB, xtest)>1.5)+1;
%             y_hat(t,4)=(yhatlas>1.5)+1;
            y_hat(t,1)=LLpred(t,1);
            %y_hat(t,1)=Train_Label(t,1);
            y_hat(t,2)=LLpred(t,2);
            y_hat(t,3)=LLpred(t,3);
            y_hat(t,4)=LLpred(t,4);
        else
            y_hat(t,1)=strategy(t,1)<50;
            y_hat(t,2)=strategy(t,2)>0;
            y_hat(t,3)=strategy(t,3)<Feature(t,end);
            y_hat(t,4)=strategy(t,4);
            y_hat(t,5)=strategy(t,5);
            y_hat(t,:)=y_hat(t,:)+1;
        end
%        y_hat(t,:)=y_hat(t,:)+1;

          
             
           Flag(:)=1;
        
             
        %% Weighted Majority Algorithm (Ensemble Learning)
        
        % Update weights
        for e = 1:No_E
            if (Flag(e) == 1)   % Only for exploitation step expert
                if (y_hat(t,e) ~= Train_Label(t))
                    w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e) = w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e)*kau;    % Punish for Miss prediction
                    %w(x(t,F_EL(1)),e) = w(x(t,F_EL(1)),e)*kau;    % Punish for Miss prediction
                end
            end
        end
        
        % Normalized w after updating
        sum_ww = sum(w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),:));
        for e = 1:No_E
            w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e) = w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e)/sum_ww;
        end
    end          
end
Predict_train=y_hat;          
partition=x(:,F_EL);
    
    %% Online Algorithm (Testing & Training)
    
    % Starting iteration for timestep
    for t = 1:T
        % Exploitation/Exploration Flag for each Expert
        Flag = zeros(No_E,1);
        %% 1. Adaptive Partitioning
        
        for e = 1:No_E
            % Find P_T of X_T
            %for k = 1:(Feature_No/No_E)
            for k = 1:feature_num_BL
                if(Test_Feature(t,F(e,k)) < P(F(e,k)))     % The boundary is P(f), Updating Section is positioned at the end of the algorithm
                    x(t,F(e,k)) = 1;
                else
                    x(t,F(e,k)) = 2;
                end
            end
        end
        

        %used the fix prediction made outside metalearner. 
        Flag(:)=1;

        if ml==false,
            t_pr=length(Train_Label);
            xtest=Test_Feature(t,:);
            trainsamepart=[];testsamepart=[];
            trainsamepart=find(partition(:,1)==x(t,1)&partition(:,2)==x(t,2)&partition(:,3)==x(t,3));
            if t>1,
                testsamepart=find(x(1:t-1,1)==x(t,1)&x(1:t-1,2)==x(t,2)&x(1:t-1,3)==x(t,3));
            end
            X=[Train_Feature(trainsamepart,:);Test_Feature(testsamepart,:)];
            Y=[Train_Label(trainsamepart,:);Test_Label(testsamepart)];


% 
%             [lasB, info] = lasso(X,Y,'NumLambda',40);
%             rfrB = TreeBagger(10,X,Y,'Method','regression'); 
%             Y=categorical(Y);
%             %logB = glmfit(X,Y);
%             logB=glmfit(X,Y,'binomial','link','logit');
%             %SVM better goes with categorical
%             svmB = fitcsvm(X,Y,'KernelFunction','rbf');
%             svmB = fitPosterior(svmB);
%             %predlog=mnrval(logB, xtest);
%             predlog=glmval(logB, xtest,'logit');
%             y_hat(t,1)=(predlog>0.5)+1;
%             score(t,1)=predlog;
%             y_hat(t,2)=predict(svmB, xtest);
%             [~,score2]=predict(svmB,xtest);
%             score(t,2)=score2(2);
% 
%             %y_hat(t,3)=cell2mat(predict(rfrB, xtest))-48;
%             score(t,3)=predict(rfrB, xtest);
%             y_hat(t,3)=(score(t,3)>1.5)+1;
%             yhatlas=info.Intercept(5)+xtest*lasB(:,5);
%             y_hat(t,4)=(yhatlas>1.5)+1;
%             score(t,4)=yhatlas;

            y_hat(t,1)=LLpred(Test_Temp(t),1);
            %y_hat(t,1)=Test_Label(t);
            y_hat(t,2)=LLpred(Test_Temp(t),2);
            y_hat(t,3)=LLpred(Test_Temp(t),3);
            y_hat(t,4)=LLpred(Test_Temp(t),4);
            %-------
            
        else
            t_pr=length(Train_Label);
            y_hat(t,1)=strategy(t+t_pr,1)<50;
            y_hat(t,2)=strategy(t+t_pr,2)>0;
            y_hat(t,3)=strategy(t+t_pr,3)<Feature(t+t_pr,end);
            y_hat(t,4)=strategy(t+t_pr,4);
            y_hat(t,5)=strategy(t+t_pr,5);
            y_hat(t,:)=y_hat(t,:)+1;
        end

               
        %% Weighted Majority Algorithm (Ensemble Learning)
        %store the weighting matrix used for current prediction
        weightobs{t}=w;
        
        if (max(Flag) == 0) % Every Expert is in exploration step
            final(t) = y_hat(t,round(No_E*rand()+0.5));     % Randomly select the final action
        else
            % If any of experts is in exploitation step
            % Find experts in Exploitation step
            sum_w = 0;
            for e = 1:No_E
                if(Flag(e) == 1)
                    sum_w = sum_w + w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e);
                end
            end
            % Weighted sum of each expert decision
            for e = 1:No_E
                if (Flag(e) == 1)
                    final_y(t) = final_y(t) + (w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e)/sum_w)*y_hat(t,e);
                end
            end
            % Decide The final action 
            if (final_y(t) > 1.5)
                final(t) = 2;   % Benign
            else
                final(t) = 1;   % Malignant
            end
            score(t,5)=final_y(t);  %record for ROC
            % Update weights
            for e = 1:No_E
                if (Flag(e) == 1)   % Only for exploitation step expert
                    if (y_hat(t,e) ~= Test_Label(t))   
                        w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e) = w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e)*kau;    % Punish for Miss prediction
                    end
                end
            end
            
            % Normalized w after updating
            sum_ww = sum(w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),:));
            for e = 1:No_E
                w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e) = w(x(t,F_EL(1)),x(t,F_EL(2)),x(t,F_EL(3)),e)/sum_ww;
            end
            
        end
   
        
        %% Adaptive Partitioning Updating
        if (Test_Label(t) == 1)    % For Malignant
            N(1) = N(1) + 1;    
            for f = 1:Feature_No
                S(f,1) = S(f,1) + Test_Feature(t,f);
                R(f,1) = S(f,1)/N(1);
            end
        else
            % For Benign
            N(2) = N(2) + 1;
            for f = 1:Feature_No
                S(f,2) = S(f,2) + Test_Feature(t,f);
                R(f,2) = S(f,2)/N(2);
            end
        end  
        % Update the boundary of partition
        for f = 1:Feature_No
            P(f) = (R(f,1)+R(f,2))/2;
        end
                
        %% Performance Analysis
        % For PER
        if (Test_Label(t) ~= final(t))
            Count(1) = Count(1) + 1;
        end
        % For FAR
        if ((Test_Label(t) == 2) && (final(t) == 1))
            Count(2) = Count(2) + 1;
        end
        % For MDR
        if ((Test_Label(t) == 1) && (final(t) == 2))
            Count(3) = Count(3) + 1;
        end
        if (Test_Label(t) == 2)
            % For FAR
            Count(4) = Count(4) + 1;
        else
            % For MDR
            Count(5) = Count(5) + 1;
        end
      
        
        %% Performance Analysis for Each Expert
        for e = 1:No_E
            % For PER
            if (Test_Label(t) ~= y_hat(t,e))
                Counting(1,e) = Counting(1,e) + 1;
            end
            % For FAR
            if ((Test_Label(t) == 2) && ( y_hat(t,e) == 1))
                Counting(2,e) = Counting(2,e) + 1;
            end
            % For MDR
            if ((Test_Label(t) == 1) && ( y_hat(t,e) == 2))
                Counting(3,e) = Counting(3,e) + 1;
            end
            if (Test_Label(t) == 2)
                % For FAR
                Counting(4,e) = Counting(4,e) + 1;
            else
                % For MDR
                Counting(5,e) = Counting(5,e) + 1;
            end
        end
                
       
    end
    Predict_test=y_hat(1:T,:);
    predict_output=[Predict_train',Predict_test'];
    partition=[partition; x(1:T,F_EL)];
    %% Print Out the Performance of Hedged bandit
    Prob_Err(l) = Count(1)/(T);
    False_Alarm(l) = Count(2)/(Count(2)+Count(4));
    Miss_Detection(l) = Count(3)/(Count(3) + Count(5));
    
    %% The performance of Best, Worst and Average Expert
     for e = 1:No_E
        Prob_Err_Exp(l,e) = Counting(1,e)/(T);
    end
    % For the Best Expert
    [Temp B_E] = min(Prob_Err_Exp(l,:));
    Best_PE(l) = Counting(1,B_E)/T;
    Best_FA(l) = Counting(2,B_E)/( Counting(2,B_E) + Counting(4,B_E));
    Best_MD(l) = Counting(3,B_E)/( Counting(3,B_E) + Counting(5,B_E));
    
    % For the Worst Expert
    [Temp W_E] = max(Prob_Err_Exp(l,:));
    Worst_PE(l) = Counting(1,W_E)/T;
    Worst_FA(l) = Counting(2,W_E)/( Counting(2,W_E) + Counting(4,W_E));
    Worst_MD(l) = Counting(3,W_E)/( Counting(3,W_E) + Counting(5,W_E)); 
    
    % For the Average Expert
    Avg_PE(l) = sum(Counting(1,:))/(No_E*T);
    Avg_FA(l) = sum(Counting(2,:))/( sum(Counting(2,:)) + sum(Counting(4,:)));
    Avg_MD(l) = sum(Counting(3,:))/( sum(Counting(3,:)) + sum(Counting(5,:))); 
    
end

%% Printing Result of Cross Validation

% Result of Weighted Majority
Result(1,1) = mean(Prob_Err);
Result(2,1) = mean(False_Alarm);
Result(3,1) = mean(Miss_Detection);

Result(4,1) = std(Prob_Err);
Result(5,1) = std(False_Alarm);
Result(6,1) = std(Miss_Detection);

% Result of Best Learner
Result(1,2) = mean(Best_PE);
Result(2,2) = mean(Best_FA);
Result(3,2) = mean(Best_MD);

Result(4,2) = std(Best_PE);
Result(5,2) = std(Best_FA);
Result(6,2) = std(Best_MD);

% Result of Worst Learner
Result(1,3) = mean(Worst_PE);
Result(2,3) = mean(Worst_FA);
Result(3,3) = mean(Worst_MD);

Result(4,3) = std(Worst_PE);
Result(5,3) = std(Worst_FA);
Result(6,3) = std(Worst_MD);

% Result of Average Learner
Result(1,4) = mean(Avg_PE);
Result(2,4) = mean(Avg_FA);
Result(3,4) = mean(Avg_MD);

Result(4,4) = std(Avg_PE);
Result(5,4) = std(Avg_FA);
Result(6,4) = std(Avg_MD);
end

%% note: 
% %%base classifier, strategy; 
% %1. momentum strategy: When the price drops below the 50-period moving average,
% %it is a confirmation that the price is trending down and a signal for
% %momentum investors to sell
% mom=zeros(3,length(features));
% mom(1,:)= features(:,end)<features(:,4);
% %2.  relative strength index:  Andrew Cardwell has developed several new in
% %terpretations of RSI to help determine and confirm trend. First, Cardwell 
% %noticed that uptrends generally traded between RSI 40 and 80, while downtr
% %ends usually traded between RSI 60 and 20
% mom(2,:)= features(:,5)<50;
% %3. volume
% mom(3,:)= features(:,6)>0;
% fn_prd=mom;